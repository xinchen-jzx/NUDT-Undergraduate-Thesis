@inproceedings {TVM18,
  author = {Tianqi Chen and Thierry Moreau and Ziheng Jiang and Lianmin Zheng and Eddie Yan and Haichen Shen and Meghan Cowan and Leyuan Wang and Yuwei Hu and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy},
  title = {{TVM}: An Automated {End-to-End} Optimizing Compiler for Deep Learning},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  year = {2018},
  isbn = {978-1-939133-08-3},
  address = {Carlsbad, CA},
  pages = {578--594},
  url = {https://www.usenix.org/conference/osdi18/presentation/chen},
  publisher = {USENIX Association},
  month = oct
}

@inproceedings{Triton19,
  author = {Tillet, Philippe and Kung, H. T. and Cox, David},
  title = {Triton: an intermediate language and compiler for tiled neural network computations},
  year = {2019},
  isbn = {9781450367196},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3315508.3329973},
  doi = {10.1145/3315508.3329973},
  abstract = {The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts – usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial. We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.},
  booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages = {10–19},
  numpages = {10},
  keywords = {neural networks, compiler, GPU},
  location = {Phoenix, AZ, USA},
  series = {MAPL 2019}
}

@misc{XLA23,
  title={Operator Fusion in XLA: Analysis and Evaluation}, 
  author={Daniel Snider and Ruofan Liang},
  year={2023},
  eprint={2301.13062},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2301.13062}, 
}

@inproceedings{AKG21,
  title={AKG: automatic kernel generation for neural processing units using polyhedral transformations},
  author={Zhao, Jie and Li, Bojie and Nie, Wang and Geng, Zhen and Zhang, Renwei and Gao, Xiong and Cheng, Bin and Wu, Chen and Cheng, Yun and Li, Zheng and Di, Peng and Zhang, Kun and Jin, Xuefeng},
  booktitle={PLDI 2021: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  year={2021},
}

@misc{IREE,
  title = {iree},
  publisher = {https://github.com/iree-org/iree},
}