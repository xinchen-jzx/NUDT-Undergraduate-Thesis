#import "thesis-template.typ": *
#import "templates/i-figured.typ"
#set heading(numbering: "1.")
#show heading: i-figured.reset-counters.with(extra-kinds: ("atom",))
#show figure: i-figured.show-figure.with(extra-prefixes: (atom: "atom:"))
#show math.equation: i-figured.show-equation

#show figure: it => {
    set text(size: 10.5pt)
    it
    v(-1em)
    box()
}

#show math.equation: set text(font: "TeX Gyre Termes Math")

#show math.equation.where(block: true): it => {
    set text(size: 10.5pt)
    it
    v(-1em)
    box()
}

#show figure.where(kind: table): it => [
  #set figure.caption(position: top)
  #it
]
// Take a look at the file `template.typ` in the file panel
// to customize this template and discover how it works.
#show: project.with(
  title: "基于RISC-V存算一体芯片的编译器关键技术研究",
  name: "简泽鑫",
  idnum: "202102001019",
  major2: "无", 
  major1: "计算机科学与技术（计算机系统）", 
  college: "计算机学院",
  grade: "2021级", 
  advisor: "曾坤",
  jobtitle: "副研究员",
  unit: "计算机学院微电子与微处理器研究所"
)

// We generated the example code below so you can see how
// your document will look. Go ahead and replace it with
// your own content!

= 绪论

== 研究背景与意义

深度学习是机器学习算法中一个非常重要的分支，近几年来发展迅速，在计算机视觉 @CV17 、自然语言处理 @NLP00 、电子商务 @e-commerce16 和药物研发 @drug18 等领域都取到了显著的成果。随着卷积神经网络（CNN） @CNN15 、循环神经网络（RNN） @RNN20 、长短期记忆网络（LSTM） @LSTM97 和生成对抗网络（GAN） @GAN14 等多种深度学习模型的出现，简化各种深度学习模型的编程对于实现其广泛应用至关重要。

同时，随着人工智能算法复杂度呈指数级跃迁以及物联网终端设备产生的数据量突破 ZB 量级，传统的计算架构正面临前所未有的“双重困境”：一方面，受限于冯诺依曼架构中存储单元与计算单元的物理分离特性，数据在片外存储与运算核心之间的数据频繁迁移带来严重的传输功耗问题。根据英特尔的研究显示，半导体工艺到了 7nm 时代，数据搬运功耗达到 35pJ/bit，占比达 63.7%。数据传输所导致的功耗损失越来越成为芯片发展的制约因素。这种“功耗墙”现象严重制约了 AI 芯片在边缘计算场景的部署能力。

另一方面，随着半导体工艺逼近物理极限，单纯依靠工艺微缩带来的性能提升已显疲态。ITRS 路线图指出，传统架构下每代工艺节点的性能增益从 28nm 时代的 40% 骤降至 5nm 节点的 15% [2]，摩尔定律的失效迫使学界寻求架构层面的突破性创新。

在此严峻形势之下，存算一体（Compute-In-Memory，CIM）架构应运而生，为突破传统架构限制带来了全新的希望与解决方案。该架构的核心创新之处在于将计算功能巧妙地集成于存储单元之中，从根源上显著减少了数据传输的庞大体量，从而成功突破了长期以来困扰业界的冯诺依曼瓶颈，实现了系统性能以及能效的大幅提升与飞跃，为计算架构领域开辟了全新的发展方向与路径。

#figure(image("images/fig3.png"), caption: "冯诺依曼 v.s. 存算一体架构")

同时，基于 RISC-V 指令集构建存算一体芯片逐渐成为 AI 加速器主流。一方面，RISC-V 指令集具有高度开放、标准化等优势，搭配上模块化设计以及强大且卓越的可定制性优势，适合用领域定制的芯片开发和设计。另一方面，可以借助 RISC-V 国际社区的力量，通过 RISC-V 扩展标准化的推动和发展促进统一、高效的 AI 编程模型和系统软件支撑框架的形成。因此，谷歌、脸书、微软等巨头，都基于 RISC-V 指令集搭建自有 AI 芯片。2023 年 RISC-V SoC 的市场渗透率达到 2.6%，市场规模 61 亿美元，并正在持续上升。

然而，人工智能深度领域定制的趋势导致了 RISC-V 存算一体芯片异构化、碎片化的特征。一方面，存算一体芯片本身具有异构性。芯片包含加速矩阵运算的张量核心以及加速向量计算的向量核心等加速部件。另一方面，虽然不同机构均基于 RISC-V 指令集进行芯片设计，但是不同机构的存算一体芯片具有迥异的架构特征，导致内部互联、存储器的存取方式等设计各不相同，造成了碎片化，给用户编程、程序优化带来了显著挑战，开发者在面向不同存算架构进行应用开发时，往往不得不针对每一种特定架构开发多个不同版本的应用程序，不仅极大地降低了开发效率，还使得应用部署过程变得异常艰难与繁琐。鉴于此，如何巧妙地实现软件操作（涵盖了计算过程、数据通信等关键环节）与硬件配置（诸如异构计算单元、存储层次结构等复杂要素）之间的深度解耦，从而使得 AI 应用开发能够摆脱对存算 IP 设计的高度依赖，已然成为破解当前“编程墙”困局的关键所在与核心突破口。

基于上述现状与需求，本课题将聚焦于面向 RISC-V 存算一体芯片的编译支持工作，针对性地对 LLVM 编译器进行深入修改与定制优化，使其能够有效支持存算指令的执行与处理。具体而言，我们将深入钻研 LLVM 编译器的架构体系以及内在工作原理，全面细致地分析 RISC-V 存算一体芯片的特性与需求，积极探索如何在 LLVM 编译框架中成功添加对 RISC-V CIM 架构的全面支持。这一过程涵盖了对指令集的合理扩展、内存模型的精准适配以及优化策略的科学调整等多个关键环节与技术要点。通过这些努力，我们旨在实现应用算子的自动化精准映射以及正确指令流的高效生成，进而更好地协调各计算部件之间的协作关系，深度挖掘芯片内部所蕴含的计算并行性优势，最终为 RISC-V 存算一体芯片构建起坚实可靠的编译支持体系，助力其在实际应用中发挥出卓越的性能表现。

== 国内外研究现状

=== 深度学习编译器

随着深度学习技术的日臻成熟，深度学习编译器领域也随之迎来了快速发展的浪潮。在这一阶段，出现了许多具有代表性的深度学习编译器：

TensorFlow XLA（Accelerated Linear Algebra/加速线性代数） @XLA23：Google于2017年开发的用于加快TensorFlow模型运行速度的编译器。其接收来自 PyTorch、TensorFlow 和 JAX 等 ML 框架的模型，在中间优化层级，XLA包括整体模型优化，如简化代数表达式、优化内存数据布局和改进调度等等。但是XLA主要针对TensorFlow优化，对其他框架的支持可能需要额外的工作；同时，其主要面向GPU和谷歌的TPU，其中间表示为深度学习算子级别的抽象，使其难以拓展到RISC-V存算一体加速器。

#figure(
  image("./assets/XLA.png", width: 50%),
  caption: [
    XLA架构图
  ],
) <XLA>

TVM（Tensor Virtual Machine） @TVM18：华盛顿大学陈天奇团队于2018年提出的面向人工智能异构加速器的编译器框架。其以 TensorFlow、PyTorch 或 ONNX 等 ML 框架导入模型，将模型编译为可链接对象模块，然后轻量级 TVM Runtime 可以用 C 语言的 API 来动态加载模型，也可以为 Python 和 Rust 等其他语言提供入口点。在中间优化层级，其提出了 Relay IR @RelayIR18 和 Tensor IR @TensorIR23 两层中间表示来进行硬件无关（如常数折叠、算符融合等）、硬件相关（如计算模式识别与加速指令生成等）的优化。TVM 可以自动为多种硬件（包括 CPU、服务器 GPU、移动端 GPU 以及基于 FPGA 的加速器）来生成优化代码，支持端到端的学习优化，并且具备灵活的编译流程，但是TVM在面对新型加速器时，不但需要开发者根据芯片指令去扩展 TVM 中的 IR，还需要根据芯片的体系结构设计去添加定向优化策略，导致其扩展性较为有限。

#figure(
  image("./assets/TVM.png", width: 100%),
  caption: [
    TVM架构图
  ],
) <TVM>

MindSpore AKG @AKG21：作为由华为主导开发并集成于其开源深度学习框架 MindSpore 中的深度学习编译器框架。AKG 可以接收来自 ML 框架的模型，生成针对特定硬件优化的内核。在中间优化层级，AKG 通过自动性能调优工具，自动生成优化的内核。同时 AKG 提供了自动化的调优过程，可以显著提高性能。然而，目前 AKG 主要针对华为的昇腾系列 AI 加速器和英伟达的 GPU进行了优化支持，对于 RISC-V 存算一体异构芯片，其支持程度相对有限，适配性欠佳。

#figure(
  image("./assets/AKG.png", width: 100%),
  caption: [
    AKG架构图
  ],
) <AKG>


Triton @Triton19：OpenAI 于 2021 年推出的编译器，主要用于加速深度学习应用在GPU上执行效率。Triton 是一种 Python DSL，专门用于编写机器学习内核，支持 CPU、GPU 和 ASIC 等多种硬件平台，具备生成针对特定硬件优化内核的能力。在中间优化层级，Triton 编译器通过块级数据流分析技术，自动优化深度学习模型的执行过程。不过，Triton 主要针对英伟达和 AMD 的 GPU 加速器进行优化，对于 RISC-V 存算一体异构芯片支持相对有限。

#figure(
  image("./assets/Triton.jpg", width: 100%),
  caption: [
    Triton架构图
  ],
) <Triton>

IREE @IREE19：Google 于 2019 年发布的一个开源的通用编译和运行时框架。通过输入高层次的机器学习模型，IREE 为各种硬件生成优化的可执行代码。在中间优化层级，IREE 利用 MLIR 进行多阶段优化，确保模型在目标平台上高效运行。IREE 提供了高性能的编译器后端，硬件抽象层允许轻松添加对新硬件的支持，但是 IREE 框架主要针对深度学习模型进行端到端的优化，而缺少统一编程模型。

#figure(
  image("./assets/IREE.svg", width: 100%),
  caption: [
    IREE架构图
  ],
) <IREE>

当下，众多深度学习编译器虽已面世，但依旧存在诸多局限性。例如，TVM 将算法定义与调度策略分离，而其调度策略需要手动编写或依赖现有模板调用，这对于缺乏编译优化专业知识的深度学习研究人员而言，使用难度较大。MindSpore AKG 能够自动生成优化的计算图，然而其优化重点在于为算子生成高性能 kernel，算子间的并行性尚未得到充分发掘等等。下表对上述深度学习编译器进行了对比总结。

#figure(
  table(
  columns: 3,
  stroke: (x: none),
  align: horizon,

  [*工作分类研究*], [*核心思想*], [*关键特征*],

  [*TVM*],
  [将机器学习模型自动编译成可供不同硬件执行的机器语言],
  [算子融合与图优化、量化技术、优化调度、Relay IR、代码生成和后端部署等],

  [*Triton*],
  [简化GPU上执行的复杂操作的开发，提供比 CUDA 更高的生产力],
  [基于分块的编程范式、灵活的 DSL 以及自动性能调优。它允许用户编写高效的内核，同时不必关心底层硬件细节],

  [*XLA*],
  [将 TensorFlow 图编译成一系列专门为给定模型生成的计算内核，从而利用模型专属信息进行优化],
  [操作融合、内存优化和专用内核生成],

  [*IREE*],
  [提供模块化和可扩展的编译器流水线，支持从高级中间表示到硬件特定执行的全流程],
  [对不同硬件的兼容性、高效的内存管理以及对实时应用的支持],

  [*AKG*],
  [通过自动化的方式来探索不同的算法实现和调度策略，找到最优的执行方案],
  [自动调优、多硬件支持和高性能内核生成],
), caption: [国内外代表性工作总结]
)

=== 深度学习加速器

随着深度学习技术的广泛应用，为其算法定制硬件加速器已成为了学术界与工业界的研究热点，当前，深度学习加速器主要沿着以下两个方向逐步发展：

其中一个是沿用传统的计算架构来提高硬件的加速性能，如 GPU、AISC、FPGA 等。寒武纪在 2014 年到 2016 年间陆续发表了 DIANNAO 系列论文 @DianNao16 ，提出了一系列全定制 AI 加速器的设计方案，通过优化数据流和存储结构，显著提升了深度学习算法的执行速度，为后续深度学习加速器的研究和开发提供了重要的理论基础和设计思路；Google于 2016 年提出一种以脉动阵列为计算核心加速矩阵运算的 AI 加速器 TPU @TPU16 ；同时 Yu-Hsin Chen 等人针对缓存与内存之间大量数据搬移问题设计了一种具有可重配置功能的深度学习加速器 Eyeriss @Eyeriss17 ，主要通过行固定（Row Stationary，RS）等方法来降低数据搬运带来的延迟和能耗开销，之后又提出了一种用于紧凑神经网络模型的加速器 Eyeriss v2 @Eyerissv219 ；清华的 thinker 团队则提出了一种基于 CGRA 的可重构加速器 @CGRA18 ，该加速器可以通过对计算引擎单元阵列进行动态配置，实现以相同的硬件支持包括卷积在内的大多数神经网络运算。

加速器的另外一个发展方向是颠覆传统的冯诺依曼架构。2010 年惠普实验室的 Williams 教授团队用忆阻器实现简单布尔逻辑功能 @Memristive10 ；2016 年，美国加州大学圣塔芭芭拉分校的谢源教授团队提出使用 RRAM 构建存算一体架构的深度学习神经网络（PRIME），首次验证了基于浮栅晶体管的存内计算在深度学习应用中的效用，相较于传统冯诺伊曼架构的传统方案，PRIME 可以实现功耗降低约 20 倍、速度提升约 50 倍，引起产业界广泛关注 @PRIME16 。

== 论文的主要研究工作

本文的主要工作是构建一个可以面向 RISC-V SRAM 存算一体芯片进行自动后端优化的编译器系统。编译器对应用程序进行应用特征分析，识别出可以加速的计算部分，并转为特定的 RISC-V 加速指令，充分利用 RISC-V 通用核心、NPU 加速核心的高效能特征。本文的主要研究工作包括以下几个方面：

1. 对本文基于的存算加速器进行了编译器架构的设计，具体设计中通过智能识别 NPU 指令和指令动态调度，实现高效的数据复用和全局数据流分析，提升整体 AI 应用的性能。

2. 设计并实现基于 RISC-V 存算一体芯片的编译器后端，提出对应的内存管理以及代码生成方案，同时利用存算一体芯片的硬件特性，降低算子内的数据搬运，减少对 SRAM 频繁写入。

== 论文的章节安排

全文共包含 6 个章节，全部章节的安排内容如下：

第一章为绪论。最后为本文的研究工作和章节安排。

第二章为主要技术基础。介绍了本文研究所涉及的相关基础概念，首先介绍了 RISC-V 基础指令集和扩展指令集，然后介绍了存算一体加速器的总体架构，最后还阐述了 LLVM 的结构和其后端的大致编译流程。

第三章为指令动态调度。.........充分利用RISC-V已有指令集实现在执行AI任务运行时对各类计算资源的灵活调度，充分发挥SRAM存算一体阵列高能效、高算力密度的硬件优势。

第四章为基于 RISC-V 存算一体芯片的编译器后端设计。......

第五章为编译器测试与分析。实验对所实现编译器的各个模块进行了正确性验证，同时选取了神经网络中比较常见的 20 种算子在 RISC-V 存算一体模拟器中性能表现。

第六章为工作总结与展望。总结了本文的研究工作，并指出了编译器的不足之处和今后需要改进完善的方向。

#pagebreak()

= 主要技术基础

== RISC-V

RISC-V 是一种基于精简指令集计算（Reduced Instruction Set Computing，RISC）原则的开源指令集架构，2010 年始于加州大学伯克利分校。它的出现意图解决现有的指令集结构（如 X86、ARM、MIPS 等）的不合理设计。相较而言，其开源特性和模块化的架构保证了设计的灵活性和高效性，以满足各种不同应用场景。架构指令集方面，RISC-V 除标准功能设计指令外，包含实现多个不同功能的可选扩展指令。设计人员可以根据实际设计要求选择基础指令集和多个扩展指令集组合，并结合硬件平台组件扩展处理器的功能范围。

RISC-V 共有 5 种基础指令集 @RISC-V ，指令空间涵盖不同位宽的指令格式，分别是弱内存次序指令集（RVWMO）、32 位整数指令集（RV32I）、32 位嵌入式整数指令集（RV32E）、64 位整数指令集（RV64I）、128 位整数指令集（RV128I）。在基础指令集的基础上 RISC-V 通过对指令集的架构设计的冗余指令进行分类，以提供扩展非标准架构指令的能力，为更专业的硬件提供设计余量。它为处理器设计中的特殊领域结构预留了指令编码空间，用户可以方便地扩展指令子集。如 @RV-ISA 所示，RISC-V 体系结构在 32/64 位指令中保留 4 组自定义指令类型，分别是 Custom-0、Custom-1、Custom-2/rv128、Custom-3/rv128。

#figure(
  image("./images/rv-isa.png", width: 100%),
  caption: [
    RISC-V指令集格式
  ],
) <RV-ISA>

根据 RISC-V 体系结构说明，用户自定义指令空间 Custom-0 和 Custom-1 被保留，不会用做标准扩展指令。而标记为 custom-2/rv128 和 custom-3/rv128 的操作码保留供未来 rv128 使用，标准扩展也会回避使用，以供用户进行指令扩展。

== 存算一体架构



== LLVM编译器

LLVM @LLVM04 是一个开源的编译器基础设施项目，它以"Low-Level Virtual Machine"的缩写命名，尽管名称中包含了"虚拟机"一词，但 LLVM 不仅仅是一个虚拟机，而是一个综合的编译器工具链。LLVM 提供了一套通用的工具和库，用于开发编译器、优化器、代码生成器等。LLVM 的核心思想是基于中间表示（Intermediate Representation，IR），它定义了一种与机器和语言无关的中间代码表示形式。LLVM IR 是一种低级别的静态单赋值（Static Single Assignment，SSA）形式，它使用基本块和指令的层次结构来表示程序的结构和行为。

=== LLVM结构

LLVM 框架主要由前端、中端、后端三大部分组成：

前端（Front End）阶段负责将高级编程语言（如 C、C++、Objective-C、Swift 等）的源代码转换为 LLVM 中间表示（LLVM IR）。这一过程涉及词法分析、语法分析、语义分析等操作，把高级语言的代码解析成编译器能够理解和处理的形式。

中端（Middle End）阶段主要对 LLVM IR 进行优化处理，目的是提高代码的质量和执行效率。优化操作包括但不限于消除无用代码、常量折叠、公共子表达式消除、循环优化等等。中端的优化是与目标硬件平台无关的，它只关注 LLVM IR 本身的优化，不涉及具体的机器指令生成。

后端（Back End）阶段将经过优化的 LLVM IR 转换为目标硬件平台能够执行的机器码。后端需要了解目标硬件的指令集架构、寄存器分配、内存布局等细节，根据这些信息将 LLVM IR 映射为相应的机器指令。同时，后端也会进行一些与硬件相关的优化，如指令调度、寄存器分配优化等，以充分发挥目标硬件的性能。LLVM 后端支持多种不同的硬件平台，包括 x86 架构的处理器、ARM 架构的处理器、PowerPC、MIPS、RISC-V 等，还包括一些新兴的专用硬件加速器。

#figure(
  image("./images/LLVM.png", width: 100%),
  caption: [
    LLVM编译器的结构
  ],
) <LLVM>

可以看到，若需引入新的编程语言，仅需开发相应的前端，让前端能够生成 LLVM IR 结构，就可以利用 LLVM 框架的相关优化。若要使编译器支持新型硬件设备，只需针对该硬件架构实现一个 LLVM 后端，将 LLVM 的中间表示（IR）转换为目标设备的机器码即可。

=== 

== 本章小结

本章主要描述了论文所涉及的相关技术基础，首先介绍了深度学习相关的基本概念，如


#pagebreak()

= 基于 RISC-V 存算一体加速器的编译器设计

本课题旨在解决异构 RISC-V 处理器的“编程墙”难题，以高能效的边缘场景为切入点，设计了一个可以面向基于 RISC-V SRAM 存算一体芯片进行自动后端优化的编译器系统，并以 RISC-V

。编译器将应用程序翻译到 LLVM IR 中间表示，并在该中间表示上进行应用特征分析，识别出可加速的 LLVM IR 范式，以自动将它们卸载到 CIM 加速器的 NPU 核心进行加速计算，转为特定的 RISC-V 加速指令，充分利用 RISC-V 通用核心、NPU 加速核心的高效能特征。

== 本章小结

#pagebreak()

= 智能识别 NPU 指令

上一章主要对本文编译器的整体架构进行了系统的阐述，本章着重介绍编译器如何在 LLVM IR 中间表示上进行应用特征分析，识别出可加速的 LLVM IR 范式，以自动将它们卸载到 CIM 加速器的 NPU 核心进行加速计算，转为特定的 RISC-V 加速指令，充分利用 RISC-V 通用核心、NPU 加速核心的高效能特征。

== LLVM IR

LLVM IR 是 LLVM 编译器框架中的一种中间语言，它提供了一个抽象层次，使得编译器能够在多个阶段进行优化和代码生成。LLVM IR 具有类精简指令集、使用三地址指令格式的特征，使其在编译器设计中非常强大和灵活。LLVM IR 的设计理念类似于精简指令集（RISC），这意味着它倾向于使用简单且数量有限的指令来完成各种操作。其指令集支持简单指令的线性序列，比如加法、减法、比较和条件分支等。这使得编译器可以很容易地对代码进行线性扫描和优化。

== 可加速范式



== 本章小结

本章主要实现了以 RISC-V 存算一体模拟器为目标设备的编译器后端。

#pagebreak()

= 指令动态调度

如何在 CPU 和 NPU 异构计算单元之间进行指令的动态调度进行了阐述，本章则介绍编译器的后端设计。后端是编译器中与体系结构密切相关的部分，本章以课题组已有的 RISC-V 存算一体模拟器为目标设备实现了一个编译器后端，并提出了相应的计算管理方案、内存管理方案以及代码生成方案，后端设计的目的是智能识别 NPU 指令并将其映射到硬件的执行单元，实现高效的数据复用和全局数据流分析，提升整体 AI 应用的性能。

== 本章小结

#pagebreak()

= 编译器测试与分析

#figure(
  table(
  columns: 2,
  stroke: (x: none),
  align: horizon,

  [*算子类别*], [*举例*],

  [*逐元素操作算子*],
  [Add, Multiply, Equal, And, Quantization],

  [*乘累加算子*],
  [Conv, GEMM, Full_Connect],

  [*激活函数算子*],
  [Exp, Sigmoid, Tanh, ReLu, Leaky_ReLu, Softmax],

  [*归一化算子*],
  [Layer_Normalization],

  [*数据排布算子*],
  [ReduceMax, ArgueMax, Transpose, Clip, Max_Pooling],
), caption: [测试算子类别表]
)

#figure(
  table(
  columns: 4,
  stroke: (x: none),
  align: horizon,

  [*算子名称*], [*CPU*], [*CPU + NPU*], [*加速比*],

  [*Add*],
  [818412],
  [28709],
  [28.51],

  [*Multiply*],
  [769260],
  [28709],
  [26.8],

  [*Equal*],
  [1457989],
  [28709],
  [50.79],

  [*And*],
  [1351485],
  [28709],
  [47.08],

  [*Quantization*],
  [1150040],
  [98922],
  [11.63],

  [*Conv*],
  [6353602],
  [8434],
  [753.34],

  [*GEMM*],
  [694459],
  [3019],
  [230.02],

  [*Full_Connect*],
  [1047333],
  [3339],
  [313.67],

  [*Exp*],
  [33307840],
  [1878977],
  [17.73],

  [*Sigmoid*],
  [7314928],
  [1878977],
  [3.89],

  [*Tanh*],
  [36234072],
  [1878977],
  [19.28],

  [*ReLu*],
  [962011],
  [18080],
  [53.21],

  [*Leaky_ReLu*],
  [986634],
  [76925],
  [12.83],

  [*Softmax*],
  [30612059],
  [2008261],
  [15.24],

  [*Layer_Normalization*],
  [1391198],
  [89989],
  [15.46],

  [*ReduceMax*],
  [813208],
  [15937],
  [51.03],

  [*ArgueMax*],
  [791174],
  [326694],
  [2.42],

  [*Transpose*],
  [794094],
  [1782],
  [445.62],

  [*Clip*],
  [987623],
  [34087],
  [28.97],

  [*Max_Pooling*],
  [826146],
  [20145],
  [41.01],
), caption: [算子测试结果对比（单位：Cycle）]
)

== 本章小结

#pagebreak()

= 总结与展望

== 本文的工作总结


== 未来的工作展望




Let $a$, $b$, and $c$ be the side
lengths of right-angled triangle.
Then, we know that:
$ a^2 + b^2 = c^2 $

Prove by induction:
$ sum_(k=1)^n k = (n(n+1)) / 2 $