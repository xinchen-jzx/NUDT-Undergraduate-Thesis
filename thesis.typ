#import "thesis-template.typ": *
#import "templates/i-figured.typ"

#import "algo.typ": algo, i, d, comment, code

#set heading(numbering: "1.")
#show heading: i-figured.reset-counters.with(extra-kinds: ("atom",))
#show figure: i-figured.show-figure.with(extra-prefixes: (atom: "atom:"))
#show math.equation: i-figured.show-equation

#show figure: it => {
    set text(size: 10.5pt)
    it
    v(-1em)
    box()
}

#show math.equation: set text(font: "TeX Gyre Termes Math")

#show math.equation.where(block: true): it => {
    set text(size: 10.5pt)
    it
    v(-1em)
    box()
}

#show figure.where(kind: table): it => [
  #set figure.caption(position: top)
  #it
]
// Take a look at the file `template.typ` in the file panel
// to customize this template and discover how it works.
#show: project.with(
  title: "基于RISC-V存算一体芯片的编译器关键技术研究",
  name: "简泽鑫",
  idnum: "202102001019",
  major2: "无", 
  major1: "计算机科学与技术（计算机系统）", 
  college: "计算机学院",
  grade: "2021级", 
  advisor: "曾坤",
  jobtitle: "副研究员",
  unit: "计算机学院微电子与微处理器研究所"
)

// We generated the example code below so you can see how
// your document will look. Go ahead and replace it with
// your own content!

= 绪论

== 研究背景与意义

深度学习是机器学习算法中一个非常重要的分支，近几年来发展迅速，在计算机视觉 @CV17 、自然语言处理 @NLP00 、电子商务 @e-commerce16 和药物研发 @drug18 等领域都取到了显著的成果。随着卷积神经网络（CNN） @CNN15 、循环神经网络（RNN） @RNN20 、长短期记忆网络（LSTM） @LSTM97 和生成对抗网络（GAN） @GAN14 等多种深度学习模型的出现，简化各种深度学习模型的编程对于实现其广泛应用至关重要。

同时，随着人工智能算法复杂度呈指数级跃迁以及物联网终端设备产生的数据量突破 ZB 量级，传统的计算架构正面临前所未有的“双重困境”：一方面，受限于冯诺依曼架构中存储单元与计算单元的物理分离特性，数据在片外存储与运算核心之间的数据频繁迁移带来严重的传输功耗问题。根据英特尔的研究显示，半导体工艺到了 7nm 时代，数据搬运功耗达到 35pJ/bit，占比达 63.7%。数据传输所导致的功耗损失越来越成为芯片发展的制约因素。这种“功耗墙”现象严重制约了 AI 芯片在边缘计算场景的部署能力。

另一方面，随着半导体工艺逼近物理极限，单纯依靠工艺微缩带来的性能提升已显疲态。ITRS 路线图指出，传统架构下每代工艺节点的性能增益从 28nm 时代的 40% 骤降至 5nm 节点的 15% [2]，摩尔定律的失效迫使学界寻求架构层面的突破性创新。

在此严峻形势之下，存算一体（Compute-In-Memory，CIM）架构应运而生，为突破传统架构限制带来了全新的希望与解决方案。该架构的核心创新之处在于将计算功能巧妙地集成于存储单元之中，从根源上显著减少了数据传输的庞大体量，从而成功突破了长期以来困扰业界的冯诺依曼瓶颈，实现了系统性能以及能效的大幅提升与飞跃，为计算架构领域开辟了全新的发展方向与路径。

#figure(image("images/fig3.png"), caption: "冯诺依曼 v.s. 存算一体架构")

同时，基于 RISC-V 指令集构建存算一体芯片逐渐成为 AI 加速器主流。一方面，RISC-V 指令集具有高度开放、标准化等优势，搭配上模块化设计以及强大且卓越的可定制性优势，适合用领域定制的芯片开发和设计。另一方面，可以借助 RISC-V 国际社区的力量，通过 RISC-V 扩展标准化的推动和发展促进统一、高效的 AI 编程模型和系统软件支撑框架的形成。因此，谷歌、脸书、微软等巨头，都基于 RISC-V 指令集搭建自有 AI 芯片。2023 年 RISC-V SoC 的市场渗透率达到 2.6%，市场规模 61 亿美元，并正在持续上升。

然而，人工智能深度领域定制的趋势导致了 RISC-V 存算一体芯片异构化、碎片化的特征。一方面，存算一体芯片本身具有异构性。芯片包含加速矩阵运算的张量核心以及加速向量计算的向量核心等加速部件。另一方面，虽然不同机构均基于 RISC-V 指令集进行芯片设计，但是不同机构的存算一体芯片具有迥异的架构特征，导致内部互联、存储器的存取方式等设计各不相同，造成了碎片化，给用户编程、程序优化带来了显著挑战，开发者在面向不同存算架构进行应用开发时，往往不得不针对每一种特定架构开发多个不同版本的应用程序，不仅极大地降低了开发效率，还使得应用部署过程变得异常艰难与繁琐。鉴于此，如何巧妙地实现软件操作（涵盖了计算过程、数据通信等关键环节）与硬件配置（诸如异构计算单元、存储层次结构等复杂要素）之间的深度解耦，从而使得 AI 应用开发能够摆脱对存算 IP 设计的高度依赖，已然成为破解当前“编程墙”困局的关键所在与核心突破口。

基于上述现状与需求，本课题将聚焦于面向 RISC-V 存算一体芯片的编译支持工作，针对性地对 LLVM 编译器进行深入修改与定制优化，使其能够有效支持存算指令的执行与处理。具体而言，我们将深入钻研 LLVM 编译器的架构体系以及内在工作原理，全面细致地分析 RISC-V 存算一体芯片的特性与需求，积极探索如何在 LLVM 编译框架中成功添加对 RISC-V CIM 架构的全面支持。这一过程涵盖了对指令集的合理扩展、内存模型的精准适配以及优化策略的科学调整等多个关键环节与技术要点。通过这些努力，我们旨在实现应用算子的自动化精准映射以及正确指令流的高效生成，进而更好地协调各计算部件之间的协作关系，深度挖掘芯片内部所蕴含的计算并行性优势，最终为 RISC-V 存算一体芯片构建起坚实可靠的编译支持体系，助力其在实际应用中发挥出卓越的性能表现。

== 国内外研究现状

=== 深度学习编译器

随着深度学习技术的日臻成熟，深度学习编译器领域也随之迎来了快速发展的浪潮。在这一阶段，出现了许多具有代表性的深度学习编译器：

TensorFlow XLA（Accelerated Linear Algebra/加速线性代数） @XLA23：Google 于 2017 年开发的一个专门针对特定领域的线性代数编译器，旨在加速 AI 框架下 TensorFlow 中的计算过程，核心思想是通过对计算图进行优化和编译，以实现更高效的计算。其接收来自 PyTorch、TensorFlow 和 JAX 等 ML 框架的模型，在中间优化层级，XLA 包括整体模型优化，如简化代数表达式、优化内存数据布局和改进调度等等。但是 XLA 主要针对 TensorFlow 优化，对其他框架的支持可能需要额外的工作；同时，其主要面向 GPU 和谷歌的 TPU，其中间表示为深度学习算子级别的抽象，使其难以拓展到 RISC-V 存算一体加速器。

#figure(
  image("./assets/XLA.png", width: 50%),
  caption: [
    XLA架构图
  ],
) <XLA>

TVM（Tensor Virtual Machine） @TVM18：华盛顿大学陈天奇团队于 2018 年提出的开源的深度学习编译器堆栈，旨在通过对神经网络模型的端到端优化，使其在各种硬件平台上高效执行。其以 TensorFlow、PyTorch 或 ONNX 等 ML 框架导入模型，将模型编译为可链接对象模块，然后轻量级 TVM Runtime 可以用 C 语言的 API 来动态加载模型，也可以为 Python 和 Rust 等其他语言提供入口点。在中间优化层级，其提出了 Relay IR @RelayIR18 和 Tensor IR @TensorIR23 两层中间表示来进行硬件无关（如常数折叠、算符融合等）、硬件相关（如计算模式识别与加速指令生成等）的优化。TVM 可以自动为多种硬件（包括 CPU、服务器 GPU、移动端 GPU 以及基于 FPGA 的加速器）来生成优化代码，支持端到端的学习优化，并且具备灵活的编译流程，但是TVM在面对新型加速器时，不但需要开发者根据芯片指令去扩展 TVM 中的 IR，还需要根据芯片的体系结构设计去添加定向优化策略，导致其扩展性较为有限。

#figure(
  image("./assets/TVM.png", width: 100%),
  caption: [
    TVM架构图
  ],
) <TVM>

MindSpore AKG @AKG21：作为由华为主导开发并集成于其开源深度学习框架 MindSpore 中的深度学习编译器框架。AKG 可以接收来自 ML 框架的模型，生成针对特定硬件优化的内核。在中间优化层级，AKG 通过自动性能调优工具，自动生成优化的内核。同时 AKG 提供了自动化的调优过程，可以显著提高性能。然而，目前 AKG 主要针对华为的昇腾系列 AI 加速器和英伟达的 GPU进行了优化支持，对于 RISC-V 存算一体异构芯片，其支持程度相对有限，适配性欠佳。

#figure(
  image("./assets/AKG.png", width: 100%),
  caption: [
    AKG架构图
  ],
) <AKG>


Triton @Triton19：OpenAI 于 2021 年推出的编译器，主要用于加速深度学习应用在GPU上执行效率。Triton 是一种 Python DSL，专门用于编写机器学习内核，支持 CPU、GPU 和 ASIC 等多种硬件平台，具备生成针对特定硬件优化内核的能力。在中间优化层级，Triton 编译器通过块级数据流分析技术，自动优化深度学习模型的执行过程。不过，Triton 主要针对英伟达和 AMD 的 GPU 加速器进行优化，对于 RISC-V 存算一体异构芯片支持相对有限。

#figure(
  image("./assets/Triton.jpg", width: 100%),
  caption: [
    Triton架构图
  ],
) <Triton>

IREE @IREE19：Google 于 2019 年发布的一个开源的通用编译和运行时框架。通过输入高层次的机器学习模型，IREE 为各种硬件生成优化的可执行代码。在中间优化层级，IREE 利用 MLIR 进行多阶段优化，确保模型在目标平台上高效运行。IREE 提供了高性能的编译器后端，硬件抽象层允许轻松添加对新硬件的支持，但是 IREE 框架主要针对深度学习模型进行端到端的优化，而缺少统一编程模型。

#figure(
  image("./assets/IREE.svg", width: 100%),
  caption: [
    IREE架构图
  ],
) <IREE>

当下，众多深度学习编译器虽已面世，但依旧存在诸多局限性。例如，TVM 将算法定义与调度策略分离，而其调度策略需要手动编写或依赖现有模板调用，这对于缺乏编译优化专业知识的深度学习研究人员而言，使用难度较大。MindSpore AKG 能够自动生成优化的计算图，然而其优化重点在于为算子生成高性能 kernel，算子间的并行性尚未得到充分发掘等等。下表对上述深度学习编译器进行了对比总结。

#figure(
  table(
  columns: 3,
  stroke: (x: none),
  align: horizon,

  [*工作分类研究*], [*核心思想*], [*关键特征*],

  [*TVM*],
  [将机器学习模型自动编译成可供不同硬件执行的机器语言],
  [算子融合与图优化、量化技术、优化调度、Relay IR、代码生成和后端部署等],

  [*Triton*],
  [简化GPU上执行的复杂操作的开发，提供比 CUDA 更高的生产力],
  [基于分块的编程范式、灵活的 DSL 以及自动性能调优。它允许用户编写高效的内核，同时不必关心底层硬件细节],

  [*XLA*],
  [将 TensorFlow 图编译成一系列专门为给定模型生成的计算内核，从而利用模型专属信息进行优化],
  [操作融合、内存优化和专用内核生成],

  [*IREE*],
  [提供模块化和可扩展的编译器流水线，支持从高级中间表示到硬件特定执行的全流程],
  [对不同硬件的兼容性、高效的内存管理以及对实时应用的支持],

  [*AKG*],
  [通过自动化的方式来探索不同的算法实现和调度策略，找到最优的执行方案],
  [自动调优、多硬件支持和高性能内核生成],
), caption: [国内外代表性工作总结]
)

=== 深度学习加速器

随着深度学习技术的广泛应用，为其算法定制硬件加速器已成为了学术界与工业界的研究热点，当前，深度学习加速器主要沿着以下两个方向逐步发展：

其中一个是沿用传统的计算架构来提高硬件的加速性能，如 GPU、AISC、FPGA 等。寒武纪在 2014 年到 2016 年间陆续发表了 DIANNAO 系列论文 @DianNao16 ，提出了一系列全定制 AI 加速器的设计方案，通过优化数据流和存储结构，显著提升了深度学习算法的执行速度，为后续深度学习加速器的研究和开发提供了重要的理论基础和设计思路；Google于 2016 年提出一种以脉动阵列为计算核心加速矩阵运算的 AI 加速器 TPU @TPU16 ；同时 Yu-Hsin Chen 等人针对缓存与内存之间大量数据搬移问题设计了一种具有可重配置功能的深度学习加速器 Eyeriss @Eyeriss17 ，主要通过行固定（Row Stationary，RS）等方法来降低数据搬运带来的延迟和能耗开销，之后又提出了一种用于紧凑神经网络模型的加速器 Eyeriss v2 @Eyerissv219 ；清华的 thinker 团队则提出了一种基于 CGRA 的可重构加速器 @CGRA18 ，该加速器可以通过对计算引擎单元阵列进行动态配置，实现以相同的硬件支持包括卷积在内的大多数神经网络运算。

加速器的另外一个发展方向是颠覆传统的冯诺依曼架构。2010 年惠普实验室的 Williams 教授团队用忆阻器实现简单布尔逻辑功能 @Memristive10 ；2016 年，美国加州大学圣塔芭芭拉分校的谢源教授团队提出使用 RRAM 构建存算一体架构的深度学习神经网络（PRIME），首次验证了基于浮栅晶体管的存内计算在深度学习应用中的效用，相较于传统冯诺伊曼架构的传统方案，PRIME 可以实现功耗降低约 20 倍、速度提升约 50 倍，引起产业界广泛关注 @PRIME16 。

== 论文的主要研究工作

本文的主要工作是构建一个可以面向 RISC-V SRAM 存算一体芯片进行自动后端优化的编译器系统。编译器对应用程序进行应用特征分析，识别出可以加速的计算部分，并转为特定的 RISC-V 加速指令，充分利用 RISC-V 通用核心、NPU 加速核心的高效能特征。本文的主要研究工作包括以下几个方面：

1. 对本文基于的存算加速器进行了编译器架构的设计，具体设计中通过智能识别 NPU 指令和指令动态调度，实现高效的数据复用和全局数据流分析，提升整体 AI 应用的性能。

2. 设计并实现基于 RISC-V 存算一体芯片的编译器后端，提出对应的内存管理以及代码生成方案，同时利用存算一体芯片的硬件特性，降低算子内的数据搬运，减少对 SRAM 频繁写入。

== 论文的章节安排

全文共包含 6 个章节，全部章节的安排内容如下：

第一章为绪论。最后为本文的研究工作和章节安排。

第二章为主要技术基础。介绍了本文研究所涉及的相关基础概念，首先介绍了 RISC-V 基础指令集和扩展指令集，然后介绍了存算一体加速器的总体架构，最后还阐述了 LLVM 的结构和其后端的大致编译流程。

第三章为基于 RISC-V 存算一体加速器的编译器总体设计。.........充分利用RISC-V已有指令集实现在执行AI任务运行时对各类计算资源的灵活调度，充分发挥SRAM存算一体阵列高能效、高算力密度的硬件优势。

第四章为智能识别 NPU 加速指令。......

第五章为指令调度。......

第六章为编译器测试与分析。实验对所实现编译器的整体功能进行了测试，通过部署一个神经网络模型对编译器的总体功能以及编译器生成带的正确性进行了验证，同时选取了神经网络中比较常见的 20 种算子在 RISC-V 存算一体模拟器中性能表现。

第七章为工作总结与展望。总结了本文的研究工作，并指出了本文设计的编译器的不足之处和今后需要改进完善的方向。

#pagebreak()

= 主要技术基础

== RISC-V

RISC-V 是一种基于精简指令集计算（Reduced Instruction Set Computing，RISC）原则的开源指令集架构，2010 年始于加州大学伯克利分校。它的出现意图解决现有的指令集结构（如 X86、ARM、MIPS 等）的不合理设计。相较而言，其开源特性和模块化的架构保证了设计的灵活性和高效性，以满足各种不同应用场景。架构指令集方面，RISC-V 除标准功能设计指令外，包含实现多个不同功能的可选扩展指令。设计人员可以根据实际设计要求选择基础指令集和多个扩展指令集组合，并结合硬件平台组件扩展处理器的功能范围。

RISC-V 共有 5 种基础指令集 @RISC-V ，指令空间涵盖不同位宽的指令格式，分别是弱内存次序指令集（RVWMO）、32 位整数指令集（RV32I）、32 位嵌入式整数指令集（RV32E）、64 位整数指令集（RV64I）、128 位整数指令集（RV128I）。在基础指令集的基础上 RISC-V 通过对指令集的架构设计的冗余指令进行分类，以提供扩展非标准架构指令的能力，为更专业的硬件提供设计余量。它为处理器设计中的特殊领域结构预留了指令编码空间，用户可以方便地扩展指令子集。如 @RV-ISA 所示，RISC-V 体系结构在 32/64 位指令中保留 4 组自定义指令类型，分别是 Custom-0、Custom-1、Custom-2/rv128、Custom-3/rv128。

#figure(
  image("./images/rv-isa.png", width: 100%),
  caption: [
    RISC-V指令集格式
  ],
) <RV-ISA>

根据 RISC-V 体系结构说明，用户自定义指令空间 Custom-0 和 Custom-1 被保留，不会用做标准扩展指令。而标记为 custom-2/rv128 和 custom-3/rv128 的操作码保留供未来 rv128 使用，标准扩展也会回避使用，以供用户进行指令扩展。

== 存算一体架构



== LLVM编译器

LLVM @LLVM04 是一个开源的编译器基础设施项目，它以"Low-Level Virtual Machine"的缩写命名，尽管名称中包含了"虚拟机"一词，但 LLVM 不仅仅是一个虚拟机，而是一个综合的编译器工具链。LLVM 提供了一套通用的工具和库，用于开发编译器、优化器、代码生成器等。LLVM 的核心思想是基于中间表示（Intermediate Representation，IR），它定义了一种与机器和语言无关的中间代码表示形式。LLVM IR 是一种低级别的静态单赋值（Static Single Assignment，SSA）形式，它使用基本块和指令的层次结构来表示程序的结构和行为。

=== LLVM结构

LLVM 框架主要由前端、中端、后端三大部分组成：

前端（Front End）阶段负责将高级编程语言（如 C、C++、Objective-C、Swift 等）的源代码转换为 LLVM 中间表示（LLVM IR）。这一过程涉及词法分析、语法分析、语义分析等操作，把高级语言的代码解析成编译器能够理解和处理的形式。

中端（Middle End）阶段主要对 LLVM IR 进行优化处理，目的是提高代码的质量和执行效率。优化操作包括但不限于消除无用代码、常量折叠、公共子表达式消除、循环优化等等。中端的优化是与目标硬件平台无关的，它只关注 LLVM IR 本身的优化，不涉及具体的机器指令生成。

后端（Back End）阶段将经过优化的 LLVM IR 转换为目标硬件平台能够执行的机器码。后端需要了解目标硬件的指令集架构、寄存器分配、内存布局等细节，根据这些信息将 LLVM IR 映射为相应的机器指令。同时，后端也会进行一些与硬件相关的优化，如指令调度、寄存器分配优化等，以充分发挥目标硬件的性能。LLVM 后端支持多种不同的硬件平台，包括 x86 架构的处理器、ARM 架构的处理器、PowerPC、MIPS、RISC-V 等，还包括一些新兴的专用硬件加速器。

#figure(
  image("./images/LLVM.png", width: 100%),
  caption: [
    LLVM编译器的结构
  ],
) <LLVM>

可以看到，若需引入新的编程语言，仅需开发相应的前端，让前端能够生成 LLVM IR 结构，就可以利用 LLVM 框架的相关优化。若要使编译器支持新型硬件设备，只需针对该硬件架构实现一个 LLVM 后端，将 LLVM 的中间表示（IR）转换为目标设备的机器码即可。

=== 

== 本章小结

本章主要描述了论文所涉及的相关技术基础，首先介绍了深度学习相关的基本概念，如


#pagebreak()

= 基于 RISC-V 存算一体加速器的编译器设计

本课题旨在解决异构 RISC-V 处理器的“编程墙”难题，以高能效的边缘场景为切入点，设计了一个可以面向基于 RISC-V SRAM 存算一体芯片进行自动后端优化的编译器系统。本章首先介绍了本文编译器的总体结构，然后对编译器前端、中端以及后端各个模块的设计进行具体介绍。关于如何智能识别出 NPU 加速指令以及如何在 CPU 和 NPU 异构计算单元之间进行指令的动态调度将会在接下来几章具体讨论。

== 编译器总体架构

#figure(
  image("./images/DL.png", width: 65%),
  caption: [
    深度学习编译器架构
  ],
) <DLCompiler>

深度学习编译器的工作是将上层深度学习模型通过各种优化技术生成硬件平台执行所需要的指令，确保神经网络模型在硬件上高效执行，其处于深度学习框架和底层硬件之间，它提供了一种中间层，可以覆盖不同的加速器硬件。其大致的框架图如@DLCompiler 所示。

#figure(
  image("./images/overview.png", width: 80%),
  caption: [
    编译器整体架构
  ],
) <Overview>

本文所设计的基于 RISC-V 存算一体模拟器的深度学习编译器的结构，具体如@Overview 所示。鉴于 LLVM 优秀的模块化与可扩展性设计，以及其具有足够表现力的中间表示 —— LLVM IR，本文编译器采用 LLVM IR 作为中间表示形式，并将 LLVM 作为后端，来复用其强大的优化器与代码生成器，同时通过扩展 RISC-V 加速指令来实现对 RISC-V 存算一体模拟器的支持。下面基于@Overview 对本文编译器各层次的主要功能及设计思想进行简要概述。

（1）编译器前端。前端的主要作用为负责接收和处理来自不同 AI 框架的模型，并将其解析为计算图，进行初步优化。目前包括 PyTorch@PyTorch19 和 TensorFlow@TensorFlow16 在内的大多数主流深度学习框架都支持将模型导出或转为 ONNX 格式，故本文中选定 ONNX 作为本文编译器的输入格式，且 ONNX 的设计定位就是作为不同框架间或框架与工具间模型转换的中间格式，所以使用 ONNX 作为输入便能够直接或间接的支持多种框架模型。

（2）编译器中间优化器。中间优化器的主要作用为接收编译器前端构建的计算图，对其进行体系结构无关的变换与优化。本文编译器提供的计算图优化策略包括算子融合、死代码删除以及公共子表达式删除等等。通过对计算图进行等价变换，可以降低计算图的计算复杂度和空间复杂度，减少模型推理的运算时间，提升整体的计算效率。最后本文编译器将优化之后的计算图转换为 LLVM IR 输入到后端进行 NPU 加速指令的识别、指令的动态调度以及目标硬件代码生成。

（3）编译器后端。后端是编译器体系中与目标体系结构关联最为密切的部分，主要功能为做与硬件结构相关的优化以及生成针对目标体系结构的优化代码。本文以本课题组已有的 RISC-V 存算一体模拟器为目标体系结构，设计并实现了一套编译器后端架构。该后端以 LLVM IR 为输入，执行一系列与体系结构相关的变换及优化操作，包括通过对 LLVM IR 进行应用特征分析，识别出可加速的 LLVM IR 范式，以自动将它们卸载到 CIM 加速器的 NPU 核心进行加速计算以及运行时在 CPU 和 NPU 异构计算单元之间进行指令的动态调度等等，充分利用 RISC-V 通用核心、NPU 加速核心的高效能特征。


== 编译器前端

编译器前端（Compiler Frontend）主要负责接收和处理来自不同 AI 框架的模型，并将其转换为通用的中间表示（IR），进行初步优化。本文编译器选定 ONNX 模型文件格式作为本文编译器的输入格式，通过解析 ONNX 模型文件来构建计算图进行图级别优化。目前包括 PyTorch@PyTorch19 和 TensorFlow@TensorFlow16 在内的大多数主流深度学习框架都对 ONNX 有着不同程度的支持，这也便于算法模型在不同框架之间的迁移以及对编译器前端接口的设计。

#figure(
  image("./images/frontend.png", width: 100%),
  caption: [
    编译器前端工作流程图
  ],
) <frontend>

=== ONNX

ONNX@ONNX （Open Neural Network Exchange），开放神经网络交换，是一套表示深度神经网模型的开放格式，由微软和 Facebook 于 2017 年推出。ONNX 定义了一组与环境和平台均无关的标准格式，用于在各种深度学习训练和推理框架转换的一个中间表示。它定义了一种可扩展的计算图模式、运算符和标准数据类型，为不同框架提供了通用的 IR。如@yolov3-onnx 使用 Netron@Netron 对 yolov3-tiny @yolov3-tiny20 模型进行可视化。

#figure(
  image("./images/yolov3-ONNX.png", width: 100%),
  caption: [
    yolov3-tiny ONNX可视化
  ],
) <yolov3-onnx>

ONNX 使用 Protobuf 序列化数据结构来存储神经网络的权重信息。Protobuf 是一种轻便高效的结构化数据存储格式，可以用于数据的结构化和序列化，适合做数据存储或数据交换格式（与语言无关、平台无关）。下表展示了 Add 算子的简化版 ONNX 模型实例。

#let t1 = table(
  columns: 1,
  [
    ```proto
    ir_version: 10
    graph {
      node {
        input: "input1"
        input: "input2"
        output: "output"
        name: "add_node"
        op_type: "Add"
      }
    ......
    ```
  ],
)

//让整个table居中显示
#align(
  center,
  [
    #t1
  ],
)

#figure(
  table(
  columns: 2,
  stroke: (x: none),
  align: horizon,

  [*类型*], [*用途*],

  [*ModelProto*],
  [定义了整个网络的模型结构],

  [*GraphProto*],
  [定义了模型的计算逻辑，包含了构成图的节点，这些节点组成了一个有向图结构],

  [*NodeProto*],
  [定义了每个 OP 具体操作],

  [*ValueInfoProto*],
  [序列化的张量，用来保存 weight 和 bias],

  [*TensorProto*],
  [定义了输入输出形状信息和张量的唯独信息],

  [*AttributeProto*],
  [定义了 OP 中的具体参数，比如 Conv 中的 stride 和 kernel_size 等],
), caption: [ONNX结构]
) <onnx-ir>

ONNX 结构如@onnx-ir 所示，当我们将 ONNX 模型加载进来之后，得到的是一个 `ModelProto`，它包含了一些版本信息，生产者信息和一个非常重要的 `GraphProto`；在 `GraphProto` 中包含了四个关键的 repeated 数组，分别是 node（`NodeProto` 类型），input（`ValueInfoProto` 类型），output（`ValueInfoProto` 类型）和 initializer（`TensorProto` 类型），其中 node 中存放着模型中的所有计算节点，input 中存放着模型所有的输入节点，output 存放着模型所有的输出节点，initializer 存放着模型所有的权重；每个计算节点都同样会有 input 和 output 这样的两个数组，通过 input 和 output 的指向关系，我们就能够利用上述信息快速构建出一个深度学习模型的拓扑图。最后每个计算节点当中还包含了一个 `AttributeProto` 数组，用于描述该节点的属性，例如 Conv 层的属性包含 `group`，`pads` 和 `strides` 等等。

=== 构建计算图

编译器首先需要对读取的模型文件进行反序列化才可以得到对应模型在内存中的表示形式。使用 `onnx.load()` 函数加载 ONNX 模型文件，得到一个 `ModelProto` 对象，从 `ModelProto` 对象中获取 `GraphProto` 对象，然后可以遍历图中的节点（`NodeProto`）、输入（`ValueInfoProto`）、输出（`ValueInfoProto`）等。对于每个节点，可以获取其名称、类型、输入和输出名称、属性等信息，属性包含操作的具体参数。同时也可以获取模型中的权重和偏置张量，这些张量存储在图的初始器（`Initializer`）中。

根据解析到的节点和张量信息，构建一个计算图。根据节点的输入和输出关系，在计算图中添加相应的节点和边。边表示数据的流动方向，通常从一个节点的输出指向另一个节点的输入。最后将解析到的张量与计算图中的节点联系起来。

== 编译器中间优化器

编译器中间优化器是对前端构建的计算图进行与硬件结构无关的优化，在不影响计算结果的情况下降低模型的空间复杂度以及计算复杂度从而减少模型在加速器上的推理时间。中间层图优化策略中根据粒度主要分为层级优化、张量级优化和元素级优化。本文编译器接收前端构建的计算图，从层级优化出发，使用算子融合、公共子表达式删除、死代码删除等优化来对构建的计算图进行处理，来提高神经网络模型执行效率。

=== 算子融合

在编译器前端解析 ONNX 模型后可以得到对应的计算图，计算图是对算子执行过程形象的表示，假设$C=\{ N,E,I,O \}$为一个计算的计算表达，计算图是一个有向连通无环图，由节点$N$（Node）、边集$E$（Edge）、输入边$I$（Input）以及输出边$E$（Output）组成的四元组，这样的抽象使我们能够更加专心于逻辑上的处理而不用在意具体的细节。而算子融合主要通过对计算图上存在数据依赖的“生产者-消费者”算子进行融合，从而提升中间 Tensor 数据的访存局部性，以此来解决内存墙问题。

算子的融合方式非常多，不同的算子融合有着不同的算子开销，也有着不同的内存访问效率的提升。现举出几个例子进行说明。

假设我们有如@example1 左侧所示的计算图，其有 4 个算子 A，B，C，D，此时我们将 C 和 D 做算子融合（可行的话），此时可以减少一次的 kernel 开销，也减小了一次的中间数据缓存。

#figure(
  image("./images/example1.png", width: 60%),
  caption: [
    算子融合案例 1
  ],
) <example1>

如果是@example2 左侧所示的计算图，B 和 C 算子是并行执行的，但此时有两次访存，可以将 A “复制”一份分别与 B，C做融合，如@example2 右侧所示，此时我们 A，B 与 A，C 可以并发执行且只需要一次访存。

#figure(
  image("./images/example2.png", width: 60%),
  caption: [
    算子融合案例 2
  ],
) <example2>

依然还是@example2 左侧所示的计算图，此时我们可以变换一下融合的方向，即横向融合，将 B 和 C 融合后减少了一次 Kernel 调度，同时结果放在内存中，缓存效率更高。

#figure(
  image("./images/example3.png", width: 60%),
  caption: [
    算子融合案例 3
  ],
) <example3>

还是@example2 左侧所示的计算图，我们也可以将 A 和 B 进行融合，此时运算结果放在内存中，再给 C 进行运算，此时可以提高内存运算效率。

#figure(
  image("./images/example4.png", width: 60%),
  caption: [
    算子融合案例 4
  ],
) <example4>

=== 公共子表达式消除

公共子表达式消除（Common Subexpression Elimination，CSE）也称为冗余表达式消除，是普遍应用于各种编译器的经典优化技术。旨在消除程序中重复计算的公共表达式，从而减少计算量和提高执行效率。

在程序中，有时会出现多个地方使用相同的表达式进行计算，并且这些表达式的计算结果相同。重复计算这些表达式，会增加不必要的计算开销。公共表达式消除的目标就是识别出这些重复的计算，并将其提取出来，只计算一次，然后将结果保存起来供后续使用。

由于 AI 编译器中子表达式是基于计算图或图层 IR，通过在计算图中搜索相同结构的子图，简化计算图的结构，从而减少计算开销，如@CSE 中 Op3 和 Op4 都经过了相同的图结构 $\{ \{ op_1,op_2 \},op_1 arrow op_2 \}$，编译器会将相同子图的所有不同输出都连接到同一个子图，然后会在后续的死代码消除中删除其他相同的子图，从而达到简化计算图的目的，减少计算开销。

#figure(
  image("./images/CSE.png", width: 60%),
  caption: [
    公共子表达式消除案例
  ],
) <CSE>

=== 死代码删除

死代码消除（Dead Code Elimination，DCE）旨在删除程序中不会被执行的代码，从而提高程序的执行效率和资源利用率。死代码是指在程序的当前执行路径下不会被访问或执行的代码片段。AI 编译器通常是通过分析计算图，找到无用的计算节点或不可达的计算节点，然后消除这些节点。

在计算图中，不可达节点是指从输入节点通过图中的有向边无法到达的节点。如@DCE1 所示，计算图中有 A，B，C 三个算子，假设三个算子都不是输入节点。不存在一条路径从输入节点到 B 节点，所以 B 节点是不可达节点，AI 编译器会删除该节点，并删除其到可达节点的边，即边 $B arrow C$。

#figure(
  image("./images/DCE1.png", width: 60%),
  caption: [
    死代码消除案例 1
  ],
) <DCE1>

在计算图中，无用节点是指某个计算节点的结果或副作用不会对输出节点产生影响。如@DCE2 所示，计算图中有 A，B，C 三个算子，B 节点输出没有后续节点，不会对后续的计算图产生影响，所以 B 节点是无用节点，AI 编译器会将该节点删除。

#figure(
  image("./images/DCE2.png", width: 60%),
  caption: [
    死代码消除案例 2
  ],
) <DCE2>

== 编译器后端

编译器后端（Compiler Backend）负责将优化后的 IR 转换为特定硬件平台的低层次表示，并进行硬件特定优化和代码生成。由于本文基于的 RISC-V 存算一体加速器，编译器后端的主要工作是识别出优化后的 LLVM IR 中的可加速范式，以自动将它们卸载到 CIM 加速器的 NPU 核心进行加速计算，最终输出在 CIM 加速器执行所需要的指令序列。下面详细介绍本文设计的编译器中基于加速器硬件结构在后端所做的工作，关于如何智能识别 NPU 指令以及指令动态调度会在本文第 4 章和第 5 章详细介绍。

=== 内存分配管理

#figure(
  image("./images/memory.png", width: 60%),
  caption: [
    内存分配流程图
  ],
) <memory>

在智能识别出可加速的 LLVM IR 指令后，编译器需要为对应的输入张量在片上内存分配内存空间以便于加速器对计算数据的读写和操作，同时还要进行内存管理防止各张量之间地址冲突从而产生的数据覆盖。内存分配的流程如@memory 所示，在识别出优化后的 LLVM IR 中的可加速范式之后，首先初始化 NPU 核心上的片上内存，然后遍历识别出的 LLVM IR 中的可加速范式，根据输入张量的不同形状，切换 CIM 加速器的寻址方式来选择最佳的加速阵列来加速对应的计算范式，同时选择一块片上内存来为其输入张量分配实际内存大小，每次分配时只需要把当前片上内存空间的地址指针作为该输入张量的首地址，同时利用底层 RISC-V 扩展指令将数据从片外内存传输到当前片上内存，然后根据张量大小对该片上内存地址指针叠加并更新，而分配的内存空间大小由张量自身的尺寸来维护，最后重复此操作以完成对可加速 LLVM IR 范式中的张量的内存分配工作。当该计算完成时，释放对应的片上内存，同时将对应的输出结果写回到片外内存，避免造成内存资源浪费。

=== 计算逻辑管理

计算逻辑管理就是基于 RISC-V 存算一体加速器的指令集架构将计算操作转为特定的 RISC-V 加速指令以及 RISC-V 通用指令，充分利用 RISC-V 通用核心、NPU 加速核心。RISC-V 存算一体加速器可以加速的基本计算范式为矩阵相关操作、向量相关操作等等，本文编译器后端根据这些范式识别 LLVM 中间表示（IR）中是否存在与之匹配的代码片段，并进一步检查对应的计算任务的规模是否可以被加速，实现自动将原 IR 代码转换成可以在这种异构架构下执行的目标 IR 代码。后续会在第 4 章中详细介绍如何智能识别 LLVM IR 中的可加速范式。

在本小节中以最简单的向量向量操作为例子来说明如何智能识别 NPU 加速指令。

== 本章小结

本章节介绍了面向 RISC-V 存算一体加速器的编译器总体架构。在 3.1 节中提出了本文编译器的总体架构，并对编译器的组成以及各个模块的功能进行了介绍。从编译器前端、中间优化器到编译器后端进行了系统的阐述。后续会着重介绍如何智能识别 NPU 加速指令以及指令动态调度。

#pagebreak()

= 智能识别 NPU 指令

上一章主要对本文编译器的整体架构，从编译器前端、中端到后端进行了系统的阐述，本章着重介绍编译器如何在 LLVM IR 中间表示上进行应用特征分析，识别出可加速的 LLVM IR 范式，以自动将它们卸载到 CIM 加速器的 NPU 核心进行加速计算，转为特定的 RISC-V 加速指令，充分利用 RISC-V 通用核心、NPU 加速核心的高效能特征。

== LLVM IR 与可加速范式

当前基于 RISC-V 存算一体模拟器仅通过支持 RISC-V 向量、矩阵扩展指令集来进行加速计算，为了利用 RISC-V 存算一体加速器中的 NPU 核心来加速计算，编译器应该识别出 LLVM IR 中可以通过 NPU 核心加速的典型计算模式。尽管 CIM 加速器的应用场景越来越多，但 CIM 加速器可以加速的基本操作却非常有限。根据其底层硬件设计，我们识别出四种典型的可加速计算范式，即标量 - 向量操作（Scalar-Vector Multiplications，SVM）、向量 - 向量操作（Vector-Vector Multiplications，VVM）、矩阵 - 向量操作（Matrix-Vector Multiplications，MVM）、矩阵 - 矩阵操作（Matrix-Matrix Multiplications，MMM），这几种计算范式都是通过比较复杂的循环策略来实现的。因此，为了实现这些可加速范式，本文对 LLVM IR 的循环结构进行深入分析，旨在剖析不同的加速范式所对应的循环结构的特征，从而为编译器智能识别出 NPU 加速指令提供关键依据，进而充分发挥 RISC-V 存算一体加速器中 NPU 核心的加速效能。

LLVM IR 是 LLVM 编译器框架中的一种中间语言，它提供了一个抽象层次，使得编译器能够在多个阶段进行优化和代码生成。LLVM IR 具有类精简指令集、使用三地址指令格式的特征，使其在编译器设计中非常强大和灵活。LLVM IR 的设计理念类似于精简指令集（RISC），这意味着它倾向于使用简单且数量有限的指令来完成各种操作。其指令集支持简单指令的线性序列，比如加法、减法、比较和条件分支等。这使得编译器可以很容易地对代码进行线性扫描和优化。

== 标量 - 向量操作（Scalar-Vector Multiplications，SVM）

== 向量 - 向量操作（Vector-Vector Multiplications，VVM）

== 矩阵 - 向量操作（Matrix-Vector Multiplications，MVM）

对于大多数神经网络应用和图像处理应用，其中包含大量的 MVM 操作，并且它们往往会对性能和能耗产生很大的影响。一个典型的 MVM 操作通常表示为两阶段循环，在由源码编译得到的 IR 代码中，“loop.condA”和“loop.condC”分别是外层循环和内层循环的循环条件块，而“loop.bodyB”和“loop.bodyD”分别是外层和内层循环的循环体块，其中字母A-D表示各模块的标识。总之，为了识别MVM模式，必须首先识别两级嵌套循环结构，然后识别内循环中的乘法和加法指令以及对应的MAC操作，同时需要保证乘法和加法相关的数据来自向量和矩阵。

== 矩阵 - 矩阵操作（Matrix-Matrix Multiplications，MMM）

矩阵 - 矩阵操作（Matrix-Matrix Multiplications，MMM）是一个在时间和能耗方面花费都很昂贵的计算任务，但它是神经网络等应用中的一个常见操作。其识别过程与MVM类似，只需要识别IR中的三层嵌套循环结构和最内层的循环中有关两个矩阵间的MAC操作。

== 本章小结

本章主要描述了如何在 LLVM IR 中间表示上智能识别出可加速范式，并对四种典型的可加速计算范式分析了其所对应的循环结构的特征，进而编译器可以依据此来智能识别出 NPU 加速指令，充分利用 RISC-V 通用核心、NPU 加速核心的高效能特征。

#pagebreak()

= 指令调度

本章着重对如何在 CPU 和 NPU 异构计算单元之间进行指令的静态和动态调度进行了阐述，当编译器后端智能识别出 NPU 加速指令后，如何在异构架构下调度生成的 RISC-V 指令来充分发挥 SRAM 存算一体阵列高能效、高算力密度的硬件优势，是一个重要的问题。本章对此进行了系统的阐述。

== 指令调度简介

指令调度是指对程序块或过程中的操作进行排序以有效利用处理器资源的任务。指令调度的目的就是通过重排指令，提高指令级并行性，使得程序在拥有指令流水线的 CPU 上更高效的运行。指令调度优化的一个必要前提就是 CPU 硬件支持指令并行，否则，指令调度是毫无意义的。

根据指令调度发生的阶段，可以把其分为静态调度和动态调度。

（1）静态调度：发生在程序编译时期。静态调度由编译器完成，在生成可执行文件之前通过指令调度相关优化，完成指令重排。

（2）动态调度：发生在程序运行时期。需要提供相应的硬件支持，比如乱序执行（OoOE: Out-of-Order Execution），此时指令的发射顺序和执行顺序可能是不一致，但是 CPU 会保证程序执行的正确性。

无论是静态调度还是动态调度，都是通过指令重排以提高指令流水，进而提高程序执行性能。静态调度和动态调度二者相辅相成，可以弥补对方的一些天然不足，协同完成指令流水优化，提高程序性能。

== 指令调度问题与约束

指令调度受到多方面的约束，如数据依赖约束、功能部件约束、寄存器约束等，在这些约束下，寻找到最优解，降低指令流水间的 stall，就是指令调度的终极目标。

指令流水间的 stall 主要由数据型冒险、结构性冒险、控制型冒险导致。

（1）数据型冒险：当前指令的执行依赖与上一条指令执行的结果。数据型冒险共有三种：写后读（RAW）、读后写（WAR）、写后写（WAW）。数据冒险可能产生数据流依赖。
    
（2）结构型冒险：多条指令同时访问一个硬件单元的时候，由于缺少相应的资源，导致结构型冒险。

（3）控制型冒险：存在分支跳转，无法预测下一条要执行的指令，导致控制型冒险。

编译器解决上述冒险的方法就是通过插入 `NOP` 指令，增加流水间的 stall 来化解冒险。

下面简单介绍一下三种数据型冒险（即数据依赖）：

（1）写后读（RAW）：一条指令读取前一条指令的写入结果。写后读是最常见的一种数据依赖类型，这种依赖被称为真数据依赖（true dependence）。

#let t5-1 = table(
  columns: 1,
  [
    ```c
    x = 1;
    y = x;
    ```
  ],
)

#align(
  center,
  [
    #t5-1
  ],
)

#h(2em)（2）读后写（WAR）：一条指令写入数据到前一条指令的操作数。这种依赖被称为反依赖或反相关（anti dependence）。

#let t5-2 = table(
  columns: 1,
  [
    ```c
    y = x;
    x = 1;
    ```
  ],
)

#align(
  center,
  [
    #t5-2
  ],
)

#h(2em)（3）写后写（WAW）：两条指令写入同一个目标。这种依赖被称为输出依赖（output dependence）。

#let t5-3 = table(
  columns: 1,
  [
    ```c
    x = 1;
    x = 2;
    ```
  ],
)

#align(
  center,
  [
    #t5-3
  ],
)

== 静态指令调度

静态指令调度是编译器后端优化的一个非常重要的阶段。现代的处理器当中绝大部分都使用流水线结构，即指令是流水执行的，我们知道理想的流水线可能使得作业效率有成倍的提高，但是对应到计算机处理器当中的流水线，要想达到效率最优是非常困难的，这就是因为指令之间的数据依赖和结构相关所造成的，静态指令调度就是在还不知道程序某些动态信息和行为的情况下，根据所分析的指令之间依赖关系以及目标硬件架构的资源状况，对指令序列进行重排，从而减少流水线停顿，以期缩短程序的执行时间。

=== 表调度算法（List Scheduling）

本文编译器中采用表调度（List Scheduling）算法，表调度是一种贪心 + 启发式方法，用以调度基本块中的各个指令操作，是基本块中指令调度的最常见方法。基于基本块的指令调度不需要考虑程序的控制流，主要考虑数据依赖、目标体系结构指令延迟、硬件资源、流水线情况等信息。

表调度的基本思想：维护一个用来存储已经准备执行的指令的 `ready` 列表和一个正在执行指令的 `active` 列表，`ready` 列表的构建主要基于数据依赖约束和硬件资源信息；根据调度算法以周期为单位来执行具体的指令调度，包括从列表中选择及调度指令，更新列表信息。

表调度算法大致分为以下三步：

（1）根据指令间依赖，建立依赖关系图。
    
（2）根据当前指令节点到根节点的长度以及指令的 latency，计算每个指令的优先级。
    
（3）不断选择一个指令，并调度它。使用两个队列维护 `ready` 的指令和正在执行的 `active` 的指令；在每个周期，选择一个满足条件的 `ready` 的指令并调度它，更新 `ready` 队列；检查 `active` 的指令是否执行完毕，更新 `active` 列表。

=== 举个例子

假设当前 CPU 有两个计算单元，即每个周期可以执行两条指令。加法指令的 latency 为 2 cycles，其他指令为 1 cycle。以下面代码为例。

#let t5-4 = table(
  columns: 1,
  [
    ```c
    r0: a = 1;
    r1: f = a + x;
    r2: b = 7;
    r3: c = 9;
    r4: g = f + d;
    r5: d = 13;
    r6: e = 19;
    r7: h = f + c;
    r8: j = d + y;
    r9: z = -1;
    r10: JMP L1;
    ```
  ],
)

#align(
  center,
  [
    #t5-4
  ],
)

#h(2em)（1）根据数据依赖关系构建出依赖关系图。如@Example-Schedule 所示。

#figure(
  image("./images/Example-Schedule.png", width: 50%),
  caption: [
    依赖关系图
  ],
) <Example-Schedule>

（2）计算指令节点的优先级。优先级计算公式为：$$

#figure(
  image("./images/Example-Schedule-2.png", width: 50%),
  caption: [
    计算指令节点的优先级
  ],
) <Example-Schedule-2>

== 动态指令调度

所谓静态指令调度就是在编译阶段由编译器实现的指令调度，目的是通过调度尽量地减少程序执行时由于数据相关而导致的流水线暂停即处理器空转。所以静态指令调度方法也叫做编译器调度法。由于在编译阶段程序没有真正执行，有一些相关可能未被发现，这是静态指令调度根本无法解决的问题。

== 本章小结

#pagebreak()

= 编译器测试与分析

本文在第 3 章从编译器前端、中端以及后端介绍了基于 RISC-V 存算一体加速器设计的编译器的总体架构，分别在第 4 章和第 5 章着重介绍了如何智能识别 NPU 加速指令以及指令调度，来充分发挥存算一体的优势。本章主要在 RISC-V 存算一体模拟器上对深度学习网络中常见的算子开展功能性验证和性能测试，并选取自定义的 FASHION MNIST 网络模型作为实例，同时呈现最终的测试结果。

== 编译器功能测试

FASHION-MNIST 是一个替代 MNIST 手写数字集的图像数据集。它是由 Zalando 旗下的研究部门提供的，其涵盖了来自 10 种类别的共 70000 个不同商品的正面图片。FASHION-MNIST 的大小、格式和训练集/测试集划分与原始的 MNIST 完全一致。60000/10000 的训练测试数据划分，$28 * 28$ 的灰度图片。

本次实验使用的网络模型为我们自定义的 FASHION MNIST 网络模型，该网络模型使用 FASHION MNIST 数据集进行训练。实验使用的自定义的 FASHION MNIST 网络模型结构如@FASHION-MNIST 所示，ONNX 可视化部分结果如@ONNX-Model 所示。该网络模型由 3 个卷积层和 2 个全连接层构成，除最后一个全连接层外每个层的输出都使用 ReLU 函数进行激活。

#figure(
  image("./images/onnx-model.png", width: 100%),
  caption: [
    网络架构图
  ],
) <ONNX-Model>

#let t6-3 = table(
  columns: 1,
  [
    ```py
    x1 = conv2d(input_image, weight_1, bias_1)
    x2 = x1 * scale_1
    x2 = clip(x2)

    x3 = conv2d(x2, weight_2, bias_2)
    x3 = x3 * scale_2
    x4 = clip(x3)

    x5 = maxpool2d(x4, stride_1)

    x6 = conv2d(x5, weight_3, bias_3)
    x6 = x6 * scale_3
    x7 = clip(x6)

    x8 = maxpool2d(x7, stride_2)

    x9 = flatten(x8)

    xa = linear(x9)
    xa = xa * scale_4
    xb = clip(xa)

    xc = linear(xb)
    xc = xc * scale_5

    xd = log_softmax(xc)
    ```
  ],
)

#align(
  center,
  [
    #t6-3
  ],
)

#figure(
  table(
  columns: 2,
  stroke: (x: none),
  align: horizon,

  [*层类型*], [*核尺寸/步长*],

  [*Conv*],
  [$3 * 3$/$1, 1$],

  [*Conv*],
  [$3 * 3$/$1, 1$],

  [*MaxPool*],
  [$2 * 2$/$2, 2$],

  [*Conv*],
  [$3 * 3$/$1, 1$],

  [*MaxPool*],
  [$2 * 2$/$2, 2$],

  [*Flatten*],
  [-],

  [*Full_Connect*],
  [-],

  [*Full_Connect*],
  [-],

  [*LogSoftmax*],
  [-],
), caption: [自定义 FASHION MNIST 网络模型结构]
) <FASHION-MNIST>

本次实验基于 FASHION MNIST 数据集，采用自定义的 FASHION MNIST 网络模型，通过所设计的针对 RISC-V  存算一体加速器的编译器进行编译并执行推理任务，来评估编译器在该特定场景下的性能表现以及能效情况。同时，使用 Python 来模拟该网络模型，记录模型的每一层的输出以及最终输出的计算结果，对编译器的功能性进行对比验证。

经过验证对比，我们发现通过本文编译器编译 ONNX 模型和使用 Python 模拟该网络模型，其每一层的输出以及最终输出的计算结果一致，故，本文编译器的功能性得到了验证。

#let t6-1 = table(
  columns: 1,
  [
    ```txt
    * * * * Performance Analysis * * * * 
    NPU work ratio: 95%
      Off-chip Transfer ratio: 65%
      Tensor Manipulate ratio: 3%
      Matrix Processing ratio: 15%
      Vector Processing ratio: 16%
    CIM Analysis:
      CIM Compute ratio: 6.0%
      CIM Space Utilization: 69.9%
      CIM Utilization: 4.2%
      Effective Performance: 42.804GOPS @INint8-Wint8
    ```
  ],
)

#align(
  center,
  [
    #t6-1
  ],
)

#h(2em) 从性能分析结果来看，我们发现在此次推理过程中 NPU 的工作比例达到了 $95%$，矩阵处理占比 $15%$，向量处理占比 $16%$，二者合计占比 $31%$，Tensor Manipulate 占比 $3%$，这表明编译器能够有效地识别、映射中间表示到指定的张量、向量等 RISC-V 扩展指令，以通过 NPU 核心加速，并根据程序依赖生成必要的同步指令在保证 CPU 和 NPU 协同计算的正确性的同时挖掘计算的并发性，体现了编译器在指令调度和资源分配方面的良好性能。关于 Tensor Manipulate 占比之所以这么低，是因为在底层硬件设计中 Tensor Manipulate 表示的操作是片上内存之间的数据搬运，而在推理过程中片上存储之间的数据搬运很少，所以 Tensor Manipulate 的占比很低。然而，片上片外数据传输（Off-Chip Transfer）比例却占据了 $65%$，成为 NPU 内部最大耗时环节，凸显“内存墙”问题。此现象与 RISC-V 存算一体芯片的层次化存储架构相关。当我们将某些计算卸载到 CIM 加速器的 NPU 核心进行加速计算时，我们需要把对应的参数的权重、偏置等等从片外内存传输到片上内存，成为影响系统整体性能的关键瓶颈。特别是该 FASHION MNIST 网络模型最后两层都是全连接层，性能受限于该全连接层的权重搬运。

在存算一体（CIM）相关分析中，CIM 计算比例仅为 $6.0%$，利用率仅为 $4.2%$，这主要是因为片上片外数据传输（Off-Chip Transfer）比例占据了 $65%$，特别是该 FASHION MNIST 网络模型最后两层都是全连接层，性能受限于该全连接层的权重搬运，所以 CIM 的计算比例和利用率很低。CIM 空间利用率达到了 69.9%，这一数值表明 CIM 内部存储空间的利用率处于一个相对合理的水平，编译器后端内存分配管理充分利用了 CIM 的存储资源，减少了不必要的存储冗余。CIM 有效性能为 $42.804$ GOPS \@INint8 - Wint8，在 INT8 量化下，NPU 实现了 $42.8$ GOPS 有效算力。

#let t6-2 = table(
  columns: 1,
  [
    ```txt
    * * * * Power Analysis * * * *
    - - - - NPU Level - - - - 
      Spad R/W energy cost: 3491064.32pJ, ratio:44%
      PE vector energy cost: 0.0pJ, ratio:0%
      CIM R/W energy cost: 1295134.72pJ, ratio:16%
      CIM compute energy cost: 3120578.56pJ, ratio:39%
      NPU energy cost: 7906777.6pJ
      CIM Compute Energy Efficiency: 12.5TOPS/W
      NPU Energy Efficiency: 4.93TOPS/W @INint8-Wint8
    - - - - System Level - - - - 
      NPU ENERGY
        NPU energy cost: 7906777.6pJ, ratio:9%
      CPU ENERGY
        CPU energy cost: 0pJ, ratio:0%
      Off-chip ENERGY
        pSRAM energy cost: 75476480pJ, ratio:91%
      Energy Efficiency
        Total energy cost: 83383257.6pJ
        Total Energy Efficiency: 0.47TOPS/W @INint8-Wint8
    ```
  ],
)

#align(
  center,
  [
    #t6-2
  ],
)

#h(2em) 在能效分析方面，于 NPU 级别，Spad R/W 能量成本占比最高，达到了 $44%$，Scratchpad 频繁读写与高片上片外数据传输占比（Off-Chip Transfer）形成了因果关系，这凸显了芯片内部存储单元的读写操作对能量消耗的显著影响，这提示我们在未来的编译器优化过程中，需要重点关注如何降低 Spad 的读写频率或优化其读写策略，以减少这部分的能量开销。CIM R/W 能量成本占比为 $16%$，CIM 计算能量成本占比为 $39%$，这表明 CIM 的能量消耗主要集中在计算过程以及数据的读写操作上，这也与 CIM 的工作原理和特点相符合。CIM 计算能效为 $12.5$ TOPS/W，而 NPU 能效为 $4.93$ TOPS/W \@INint8 - Wint8，相比之下，CIM 在能效方面表现出了一定的优势，这也进一步证明了存算一体架构在能效提升方面的潜力，但同时也需要注意到整个 NPU 的能效还有较大的提升空间，需要综合考虑编译器的优化策略以及硬件架构的改进来进一步提高能效。

在系统级别，NPU 的能量成本占比为 $9%$，而片上片外内存传输（Off-Chip）的能量成本占比高达 $91%$，这再次凸显了片上片外数据传输对系统整体能量消耗的主导地位，这一结果与性能分析中的片上片外数据传输占比（Off-Chip Transfer）比例较高的现象相呼应，进一步强调了减少片上片外数据传输对于提升系统能效的重要性。整个系统的总能量成本为 $83383257.6$ pJ，总能效仅为 $0.47$ TOPS/W \@INint8 - Wint8，这一较低的系统能效值凸显片上片外数据传输对能效的毁灭性影响，需硬件-编译器协同设计：如采用3D堆叠内存等等。

== 编译器性能测试

本次实验不仅基于 FASHION MNIST 数据集完成了自定义网络模型的推理任务，还对一些常见的算子在利用 NPU 和不利用 NPU 两种情况下进行了详细测试与对比，性能测试结果如@result 所示，深度学习常见的算子的分类如@operator 所示。

#figure(
  table(
  columns: 2,
  stroke: (x: none),
  align: horizon,

  [*算子类别*], [*举例*],

  [*逐元素操作算子*],
  [Add, Multiply, Equal, And, Quantization],

  [*乘累加算子*],
  [Conv, GEMM, Full_Connect],

  [*激活函数算子*],
  [Exp, Sigmoid, Tanh, ReLu, Leaky_ReLu, Softmax],

  [*归一化算子*],
  [Layer_Normalization],

  [*数据排布算子*],
  [ReduceMax, ArgueMax, Transpose, Clip, Max_Pooling],
), caption: [测试算子类别表]
) <operator>

#figure(
  table(
  columns: 4,
  stroke: (x: none),
  align: horizon,

  [*算子名称*], [*CPU*], [*CPU + NPU*], [*加速比*],

  [*Add*],
  [818412],
  [28709],
  [28.51],

  [*Multiply*],
  [769260],
  [28709],
  [26.8],

  [*Equal*],
  [1457989],
  [28709],
  [50.79],

  [*And*],
  [1351485],
  [28709],
  [47.08],

  [*Quantization*],
  [1150040],
  [98922],
  [11.63],

  [*Conv*],
  [6353602],
  [8434],
  [753.34],

  [*GEMM*],
  [694459],
  [3019],
  [230.02],

  [*Full_Connect*],
  [1047333],
  [3339],
  [313.67],

  [*Exp*],
  [33307840],
  [1878977],
  [17.73],

  [*Sigmoid*],
  [7314928],
  [1878977],
  [3.89],

  [*Tanh*],
  [36234072],
  [1878977],
  [19.28],

  [*ReLu*],
  [962011],
  [18080],
  [53.21],

  [*Leaky_ReLu*],
  [986634],
  [76925],
  [12.83],

  [*Softmax*],
  [30612059],
  [2008261],
  [15.24],

  [*Layer_Normalization*],
  [1391198],
  [89989],
  [15.46],

  [*ReduceMax*],
  [813208],
  [15937],
  [51.03],

  [*ArgueMax*],
  [791174],
  [326694],
  [2.42],

  [*Transpose*],
  [794094],
  [1782],
  [445.62],

  [*Clip*],
  [987623],
  [34087],
  [28.97],

  [*Max_Pooling*],
  [826146],
  [20145],
  [41.01],
), caption: [算子测试结果对比（单位：Cycle）]
) <result>

#figure(
  image("./images/result.png", width: 100%),
  caption: [
    算子测试结果
  ],
) <Operator-Result>

从测试结果可以看出，NPU 的引入为大多数算子带来了显著的加速效果。其中，Conv 算子的加速比高达 753.34，GEMM 算子的加速比也达到了 $230.02$，这表明编译器能够有效利用 NPU 的加速计算的能力，优化卷积和矩阵运算等计算密集型操作的执行效率。此外，像 Add、Multiply、Equal 等基础算术和逻辑运算算子，以及 Leaky_ReLu、Clip 等激活函数相关的算子，其加速比也都在 $20$ 以上，充分体现了 NPU 在处理这些常见算子时的优势。

然而，对于某些特定算子，如 ArgueMax，其加速比仅为 2.42，相对较低。这是因为该算子本身的计算特性与 NPU 的架构优势不完全匹配。此外，像 Exp、Sigmoid、Tanh 等算子，虽然也取得了一定的加速效果，但加速比相较于其他算子并不算突出，这提示我们在后续的编译器优化工作中，可以重点关注这类算子，深入分析其计算模式和数据访问特点，进一步挖掘 NPU 的潜在性能。

综合来看，本次实验结果表明，本文所设计的编译器能够有效利用 NPU 的硬件资源，为大多数算子带来显著的性能提升，但针对个别特殊算子的优化仍有改进空间，以实现更广泛的算子加速效果，进而提升整个神经网络模型在 RISC-V 存算一体加速器上的执行效率。

== 本章小结

本章对编译器的整体功能进行了功能性测试和性能测试，并进行了结果分析，验证了其基本功能的完整性。实验表明本文的编译器可以成功的将深度学习模型编译为等价的 RISC-V 通用指令和 RISC-V 加速指令，充分利用 RISC-V 通用核心、NPU 加速核心的高效能特征。

#pagebreak()

= 总结与展望

== 工作总结

面向 RISC-V 芯片构建编译器方面，现有人工智能编译社区提出了统一的中间表示 MLIR，已经在 IREE、Triton 等多个 AI 编译器中得到应用。然而，这些编译器并不能感知存算一体芯片架构，以及其使用的 RISC-V 扩展指令集，也无法表征存算芯片的计算、并发、协同、通信、数据重用等特征。因此本文提出了面向 RISC-V 存算一体加速器的编译器设计与实现，经过总结，本文完成的主要工作如下：

（1）对本文所基于的 RISC-V 存算一体加速器进行了深度学习编译器的基本设计，在设计实现中可以解析 ONNX 模型为计算图，通过算子融合等优化方法减少了加速器对内存的访问以及存储空间的浪费，同时还使用内存分配地址叠加的方式避免了各张量之间的数据覆盖。

（2）智能识别 NPU 加速指令。为了在 RISC-V 存算一体加速器中利用到 NPU 核心进行加速，我们从 LLVM IR 中识别出几种典型的计算模式，这些模式不受高级编程范例的影响。我们在 LLVM IR 中间表示上进行应用特征分析，识别出可加速的 LLVM IR 范式，以自动将它们卸载到 CIM 加速器的 NPU 核心进行加速计算，转为特定的 RISC-V 加速指令，充分利用 RISC-V 通用核心、NPU 加速核心的高效能特征。

（3）指令的动态调度。在 CPU 和 NPU 异构计算单元之间进行指令的动态调度。在异构架构下，充分利用 RISC-V 已有指令集实现在执行 AI 任务运行时对各类计算资源的灵活调度，充分发挥 SRAM 存算一体阵列高能效、高算力密度的硬件优势。

== 工作展望